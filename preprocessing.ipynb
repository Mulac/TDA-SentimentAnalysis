{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "110qaIUBR_gdE4cHk40lRHHQ7WFdbkOoY",
      "authorship_tag": "ABX9TyNqX+Sk/hZvyAk08fW2osRo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mulac/TDA-SentimentAnalysis/blob/master/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM-Z4qOfG6FN",
        "colab_type": "text"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzRmloa7HA9p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "b0c6e481-9125-43e7-848d-f5e089a0194c"
      },
      "source": [
        "!pip install wordsegment\n",
        "!pip install tweet-preprocessor\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "from wordsegment import segment, load\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wordsegment in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkxz7JOiIaxn",
        "colab_type": "text"
      },
      "source": [
        "Define a dictionary of contractions and their extended form (https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/#d-removing-contractions), along with a string of punctuation to be removed. The tweet preprocessor will remove URLs, mentions, numbers and emojis. Airline names are included as stopwords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec9OvECPHDBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\", \"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\", \"so's\": \"so as\", \"this's\": \"this is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\", \"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "load()\n",
        "punctuation = \"+!?$%&()*+,'-./:;<=>[\\]^_`{|}~@\\\"\"\n",
        "p.set_options(p.OPT.URL, p.OPT.MENTION, p.OPT.NUMBER, p.OPT.EMOJI)\n",
        "airline_stopwords = [\"United\", \"AmericanAir\", \"USAirways\", \"JetBlue\", \"VirginAmerica\",\"SouthwestAir\"]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWhDAbboHHL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Each hashtag will be segmented into words.\n",
        "\"\"\"\n",
        "def sep_hashtags(tweet:list):\n",
        "  sep_hashtags = []\n",
        "  for word in tweet:\n",
        "    if word[0] == '#':\n",
        "      cleaned_hashtag = segment(word)\n",
        "      for segmented in cleaned_hashtag:\n",
        "        sep_hashtags.append(segmented)\n",
        "    else:\n",
        "      sep_hashtags.append(word)\n",
        "  return sep_hashtags"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN4-XR9rHJCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_contractions(contraction_dict):\n",
        "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
        "    return contraction_dict, contraction_re"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uChebKpHLWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "replace contractions\n",
        "\"\"\"\n",
        "def replace_contractions(text):\n",
        "    def replace(match):\n",
        "        return contractions[match.group(0)]\n",
        "    return contractions_re.sub(replace, text)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScWGffZ_G3rV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n",
        "\n",
        "  #Tokenise & clean\n",
        "  text = p.clean(text) # removes mentions, numbers, emojis, URLS \n",
        "  text = BeautifulSoup(text, features = \"lxml\").get_text()  # converts HTML entities \n",
        "  text = replace_contractions(text) # contractions are extended, e.g isn't = is not \n",
        "  text = text.translate(str.maketrans(' ', ' ', punctuation)) # punctuation (except for #) removed\n",
        "\n",
        "  lst_text = text.split() # split into a list\n",
        "  lst_text = sep_hashtags(lst_text) # hashtags are separated into words\n",
        "\n",
        "  #remove Stopwords\n",
        "  lst_text = [word for word in lst_text if word not in lst_stopwords]\n",
        "\n",
        "  #Stemming\n",
        "  if flg_stemm:\n",
        "    ps = nltk.stem.porter.PorterStemmer()\n",
        "    lst_text = [ps.stem(word) for word in lst_text]\n",
        "\n",
        "  #Lemmatisation\n",
        "  if flg_lemm:\n",
        "    lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "    lst_text = [lem.lemmatize(word) for word in lst_text]\n",
        "\n",
        "  #rejoin the string\n",
        "  text = \" \".join(lst_text)\n",
        "  return text"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTTqKaQwHPPG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "contractions, contractions_re = get_contractions(contraction_dict)\n",
        "\n",
        "df = pd.read_csv('Tweets.csv')\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: preprocess_text(x, flg_lemm=True, lst_stopwords=airline_stopwords))\n",
        "\n",
        "df.to_csv(\"ProcessedTweets.csv\")"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}
